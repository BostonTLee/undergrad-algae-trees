---
title: "DSCI 445 - Utilizing Machine Learning for Algae Predictions"
author: "Dawson Eliasen, Boston Lee, Miles Austin"
date: "12/7/2020"
output:
  beamer_presentation: default
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(engine.path='venv/bin/python3')
```

```{python, include=FALSE}
from plots import *
```

## Motivation

When looking at the amount of hand-collected (in situ) data, we see that in
Colorado only has data for the major municipal water bodies like Horsetooth.

![](map.jpeg)

  
## Monthly

![](HT_Monthly.jpeg)


## Yearly

![](HT_Obs.jpeg)


## The use of remote sensing data

Landsat data can provide us with valuable data that can show us what the
conditions of the reservoirs are at any given time. This data, like the
blueness or greenness of a reservoir, can help us look at reservoirs that we do
not have data for.

![](ht_color_dist.jpeg)


## Climatology by month

We can see that Horsetooth is more green and has higher wavelengths in the
summer months. This trend is why we only care about summer months.

![](HT_Climatology.jpeg)


## Summer Months

We can see that over time, during the summer, that Horsetooth is actually
becoming more blue. This isn't the case for all reservoirs, as some are
actually becoming more green. The greener they are, the more suspected algae is
in the water.

![](ht_color_over_time.jpeg)

## Summer Months

- Over time, during the summer, Horsetooth is becoming more blue. 
- This is not the case for all reservoirs; some are actually becoming more
  green. 
- The greener they are, the more suspected algae is in the water.

![](ht_color_over_time.jpeg)

## Methodology

```{python, echo=FALSE}
plot_histograms()
```

## Algorithm Choice

- The algae bloom data is spatiotemporal: We have measurements that are
  potentially correlated over geographic space, that are taken over time
- This means we cannot use any regression-based methods. Assumptions about
  uncorrelated errors are violated.
- Solution: Non-parametric algorithms

## Algorithm Choice, Cont.

- We chose two non-parametric algorithms:
  - Random Forest
  - Gradient Boosting
  
## Gradient Boosted Trees

- Given a differentiable loss function $L(\cdot)$, we want to **minimize** the total loss for points in the test set:

$$\sum_{i = 1}^{n} L(y_i, F(x_i))$$

Where $F(y_i, F(x_i))$ is a measure of how **wrong** $F(x_i)$ is in predicting $y_i$.

- This leads to an optimization problem, which (because of differentiability) can be solved by gradient descent.

## Gradient Boosted Trees, cont.

- We can interpret residuals as the **negative gradient** of the total loss
  defined before.
- Update $F(\cdot)$ based on the negative gradient of our total loss.
- This algorithm can handle different loss functions, with different
  properties, making it very flexible.
- Here, we use the default implementation in `scikit-learn`, which has a
  least-squares loss function:

$$L(y_i, F(x_i)) = (y_i = F(x_i))^2$$


## Random forest feature importance

- We were surprised to see `areasqkm` as the most important feature
- `dwl` as the second most important feature makes sense

```{python, echo=FALSE}
plot_feature_importance()
```


## How does the feature subset affect performance?

- Different models behave similarly
- Adding in `areasqkm` resulted in the biggest performance increase

```{python, echo=FALSE}
plot_subset_performance()
```


## How does the merge threshold affect performance?

- Different models behave similarly
- Interestingly, a merge threshold of 1 is worse than 0 and 2

```{python, echo=FALSE}
plot_thresh_performance()
```


## Results summary

- Our best model was a random forest with a merge threshold of 3 days and included all available features
- Our best model received a CV MSE of about 1.75 and a test set MSE of about 1.09


## Predicted values

```{python, echo=FALSE}
plot_predicted_values()
```
